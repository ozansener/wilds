{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "vocational-arlington",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import csv\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "decimal-expression",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "broad-associate",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_to_dict(file_name):\n",
    "    res_dict = {}\n",
    "    keym = {}\n",
    "    with open(file_name, newline='') as csvfile:\n",
    "        csv_reader = csv.reader(csvfile, delimiter=',')\n",
    "        for i, row in enumerate(csv_reader):\n",
    "            if i == 0:\n",
    "                for j, rr in enumerate(row):\n",
    "                    res_dict[rr] = []\n",
    "                    keym[j] = rr\n",
    "            else:\n",
    "                for j, rr in enumerate(row):\n",
    "                    if j not in keym:\n",
    "                        print('error')\n",
    "                        break\n",
    "                    res_dict[keym[j]].append(float(rr))\n",
    "    return res_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "criminal-sweet",
   "metadata": {},
   "outputs": [],
   "source": [
    "def early_stop(result_dict, key1, key2, max_ep):\n",
    "    if max_ep is None:\n",
    "        best_ep_0 = np.argmax(np.array(result_dict[key1]))\n",
    "        best_ep_1 = np.argmax(np.array(result_dict[key2]))\n",
    "    else:\n",
    "        best_ep_0 = np.argmax(np.array(result_dict[key1][0:max_ep]))\n",
    "        best_ep_1 = np.argmax(np.array(result_dict[key2][0:max_ep]))\n",
    "    return best_ep_0, best_ep_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "digital-young",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_results(entries, res_keys, which, max_ep):\n",
    "    map_k = {0:\"test\", 1:\"val\", 2:\"id_test\", 3: \"train\"}\n",
    "    res = {}\n",
    "    for i in range(2):\n",
    "        res[i] = {\"test\":{}, \"val\":{}, \"id_test\":{}, \"train\":{}}\n",
    "        for dic in [\"test\", \"val\", \"id_test\", \"train\"]:\n",
    "            for res_k in res_keys:\n",
    "                res[i][dic][res_k] = []\n",
    "    for entry in entries:\n",
    "        e1,e2 = early_stop(entries[entry][which],res_keys[0], res_keys[1], max_ep)\n",
    "        for i, e_v in enumerate([e1, e2]):\n",
    "            for res_k in res_keys:\n",
    "                for j_id in range(4):\n",
    "                    if entries[entry][j_id] is None:\n",
    "                        continue\n",
    "                    if e_v >= len(entries[entry][j_id][res_k]):\n",
    "                        print(\"Error\", res_k, len(entries[entry][j_id][res_k]), map_k[j_id])\n",
    "                        res[i][map_k[j_id]][res_k].append(entries[entry][j_id][res_k][-1])\n",
    "                    else:\n",
    "                        res[i][map_k[j_id]][res_k].append(entries[entry][j_id][res_k][e_v])\n",
    "\n",
    "    return res\n",
    "def print_row(row, res_keys):\n",
    "    final_str = \"\"\n",
    "    for res_k in res_keys:\n",
    "        final_str += \" {:04.2f}\".format(100*np.mean(row[res_k]))\n",
    "        final_str += \" {:04.2f}\".format(100*np.std(row[res_k]))\n",
    "    return final_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "narrow-shanghai",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_config(log_di, my_keys):\n",
    "    config = open(log_di+'log.txt').read().split('\\n')\n",
    "    dat_conf = {}\n",
    "    for ll in config:\n",
    "        if 'Epoch [0]' in ll:\n",
    "            break\n",
    "        if 'Dataset kwargs' in ll and '{}' not in ll:\n",
    "            dat_conf[ll.split(':')[0]] = ll.split(':')[2]\n",
    "        elif ll.split(':')[0] in my_keys:\n",
    "            dat_conf[ll.split(':')[0]] = ll.split(':')[1]\n",
    "    return dat_conf\n",
    "\n",
    "def parse_config_orig(log_di, my_keys):\n",
    "    config = open(log_di+'log.txt').read().split('\\n')\n",
    "    dat_conf = {}\n",
    "    for ll in config:\n",
    "        if 'Epoch [0]' in ll:\n",
    "            break\n",
    "        if ll.split(':')[0] in my_keys:\n",
    "            dat_conf[ll.split(':')[0]] = ll.split(':')[1]\n",
    "    return dat_conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "sustained-israel",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.00000000e-04, 6.81292069e-04, 4.64158883e-03, 3.16227766e-02,\n",
       "       2.15443469e-01, 1.46779927e+00, 1.00000000e+01])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(np.linspace(np.log(0.0001),np.log(10),7))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "serial-pressing",
   "metadata": {},
   "source": [
    "# iWildCam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "banner-brand",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dicts = sorted(glob.glob('./AMASK_*/'), key= lambda x:int(x.split('_')[-1].split('/')[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "north-castle",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_keys = ['Dataset', 'Algorithm', 'Uniform over groups', 'Distinct groups', 'N groups per batch']\n",
    "my_keys += ['Batch size', 'Rd type', 'Warm start epoch', 'Control only direction', 'Only inconsistent']\n",
    "my_keys += ['Without sampling', 'Lr', 'Weight decay', 'Seed']\n",
    "\n",
    "exps = {}\n",
    "for log_di in log_dicts:\n",
    "    dat_conf = parse_config_orig(log_di, my_keys)\n",
    "    if not dat_conf['Dataset'] == ' iwildcam':\n",
    "        continue\n",
    "    exp_name = \"{}_{}_{}\".format(dat_conf['Warm start epoch'], dat_conf['Rd type'], dat_conf['Lr'])\n",
    "    if exp_name not in exps:\n",
    "        exps[exp_name] = {}\n",
    "    exps[exp_name][dat_conf['Seed']] = (read_to_dict(log_di+'test_eval.csv'), \n",
    "                                        read_to_dict(log_di+'val_eval.csv'), \n",
    "                                        read_to_dict(log_di+'id_test_eval.csv'),\n",
    "                                        read_to_dict(log_di+'train_eval.csv'))\n",
    "    if '3e-05' not in exp_name:\n",
    "        continue\n",
    "    if '2_' not in exp_name or '-1_ ' in exp_name:\n",
    "        continue\n",
    "    print(log_di, exp_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "medium-borough",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "freelance-craft",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -1_ 42_ 3e-05\n",
      "Te\t ES_0  38.12 2.15 2.05 0.68\n",
      "ID \t ES_0  34.19 2.37 2.30 1.08\n",
      "Te\t ES_1  37.94 1.89 2.08 0.72\n",
      "ID \t ES_1  33.51 1.53 2.36 1.16\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for exp_id in exps:\n",
    "    #if '3e-05' not in exp_id:\n",
    "    #    continue\n",
    "    #if '2_' not in exp_id or '-1_ ' in exp_id:\n",
    "    #    continue\n",
    "    resO = collect_results(exps[exp_id], ('acc_avg', 'F1-macro_all'),0 , None)\n",
    "    print(exp_id)\n",
    "\n",
    "    for alg in resO:\n",
    "        res_test_str = \"ES_{} \".format(alg) +print_row(resO[alg]['test'], ('acc_avg', 'F1-macro_all'))\n",
    "        res_id_t_str = \"ES_{} \".format(alg) +print_row(resO[alg]['id_test'], ('acc_avg', 'F1-macro_all'))\n",
    "        res_train_str = \"ES_{} \".format(alg) +print_row(resO[alg]['train'], ('acc_avg', 'F1-macro_all'))\n",
    "\n",
    "        print(\"Te\\t\", res_test_str)\n",
    "        print(\"ID \\t\", res_id_t_str)\n",
    "        #print(\"Tr \\t\", res_train_str)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "modular-easter",
   "metadata": {},
   "source": [
    "# Py150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "purple-duncan",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dicts = sorted(glob.glob('./AMASK_*/'), key= lambda x:int(x.split('_')[-1].split('/')[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "spanish-situation",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_keys = ['Dataset', 'Algorithm', 'Uniform over groups', 'Distinct groups', 'N groups per batch']\n",
    "my_keys += ['Batch size', 'Rd type', 'Warm start epoch', 'Control only direction', 'Only inconsistent']\n",
    "my_keys += ['Without sampling', 'Lr', 'Weight decay', 'Seed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "accredited-overall",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./AMASK_27/  8e-05\n",
      "./AMASK_28/  8e-05\n",
      "./AMASK_29/  8e-05\n"
     ]
    }
   ],
   "source": [
    "exps = {}\n",
    "for log_di in log_dicts:\n",
    "    dat_conf = parse_config_orig(log_di, my_keys)\n",
    "    if not dat_conf['Dataset'] == ' py150':\n",
    "        continue\n",
    "    print(log_di, dat_conf['Lr'])\n",
    "    exp_name = \"{}_{}_{}_{}_{}\".format(dat_conf['Warm start epoch'], dat_conf['Rd type'], \n",
    "                                 dat_conf['Lr'], dat_conf['N groups per batch'],\n",
    "                                 dat_conf['Batch size'])\n",
    "    if exp_name not in exps:\n",
    "        exps[exp_name] = {}\n",
    "    exps[exp_name][dat_conf['Seed']] = (read_to_dict(log_di+'test_eval.csv'), \n",
    "                                        read_to_dict(log_di+'val_eval.csv'), \n",
    "                                        read_to_dict(log_di+'id_test_eval.csv'),\n",
    "                                        read_to_dict(log_di+'train_eval.csv'),\n",
    "                                        log_di)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "offensive-tournament",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -1_ 42_ 8e-05_ 2_ 6\n",
      "7.0\n",
      "./AMASK_27/\n",
      "6.0\n",
      "./AMASK_28/\n",
      "3.0\n",
      "./AMASK_29/\n",
      "Error Acc (Class-Method) 6 test\n",
      "Error Acc (Class-Method) 6 id_test\n",
      "Error Acc (Overall) 6 test\n",
      "Error Acc (Overall) 6 id_test\n",
      "Te\t ES_0  62.63 0.30 65.82 0.16\n",
      "Te\t ES_1  62.37 0.07 65.79 0.07\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for exp_id in exps:\n",
    "    print(exp_id)\n",
    "    #if not ('3_ 0_ 1e-05_ 2_ 6' in exp_id or '3_ 1_ 1e-05_ 2_ 6' in exp_id):\n",
    "    #    continue\n",
    "    for kk in exps[exp_id]:\n",
    "        print(np.max(exps[exp_id][kk][0]['epoch']))\n",
    "        print(exps[exp_id][kk][4])\n",
    "    resO = collect_results(exps[exp_id], ('Acc (Class-Method)', 'Acc (Overall)'),1, None)\n",
    "\n",
    "    for alg in resO:\n",
    "        res_test_str = \"ES_{} \".format(alg) +print_row(resO[alg]['test'], ('Acc (Class-Method)', 'Acc (Overall)'))\n",
    "        res_id_t_str = \"ES_{} \".format(alg) +print_row(resO[alg]['id_test'], ('Acc (Class-Method)', 'Acc (Overall)'))\n",
    "        res_train_str = \"ES_{} \".format(alg) +print_row(resO[alg]['train'], ('Acc (Class-Method)', 'Acc (Overall)'))\n",
    "\n",
    "        print(\"Te\\t\", res_test_str)\n",
    "        #print(\"ID \\t\", res_id_t_str)\n",
    "        #print(\"Tr \\t\", res_train_str)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "instrumental-transport",
   "metadata": {},
   "source": [
    "# Mol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fossil-crisis",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dicts = sorted(glob.glob('./AMASK_*/'), key= lambda x:int(x.split('_')[-1].split('/')[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "threatened-arrangement",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_keys = ['Dataset', 'Algorithm', 'Uniform over groups', 'Distinct groups', 'N groups per batch']\n",
    "my_keys += ['Batch size', 'Rd type', 'Warm start epoch', 'Control only direction', 'Only inconsistent']\n",
    "my_keys += ['Without sampling', 'Lr', 'Weight decay', 'Seed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "local-dutch",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./AMASK_13/  0.001\n",
      "./AMASK_14/  0.001\n",
      "./AMASK_15/  0.001\n"
     ]
    }
   ],
   "source": [
    "exps = {}\n",
    "for log_di in log_dicts:\n",
    "    dat_conf = parse_config(log_di, my_keys)\n",
    "    if not dat_conf['Dataset'] == ' ogb-molpcba':\n",
    "        continue\n",
    "    print(log_di, dat_conf['Lr'])\n",
    "    exp_name = \"{}_{}_{}_{}_{}\".format(dat_conf['Warm start epoch'], dat_conf['Rd type'], \n",
    "                                 dat_conf['Batch size'], dat_conf['N groups per batch'],\n",
    "                                 dat_conf['Batch size'])\n",
    "    if exp_name not in exps:\n",
    "        exps[exp_name] = {}\n",
    "    exps[exp_name][dat_conf['Seed']] = (read_to_dict(log_di+'test_eval.csv'), \n",
    "                                        read_to_dict(log_di+'val_eval.csv'), \n",
    "                                        None,\n",
    "                                        read_to_dict(log_di+'train_eval.csv'),\n",
    "                                        log_di)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "radical-professor",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "subtle-worker",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "' -1_ 42_ 32_ 4_ 32'\n",
      "4.0\n",
      "./AMASK_13/\n",
      "4.0\n",
      "./AMASK_14/\n",
      "4.0\n",
      "./AMASK_15/\n",
      "Te\t ES_0  4.87 0.10 4.87 0.10\n",
      "Tr \t ES_0  3.20 0.15 3.20 0.15\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for exp_id in exps:\n",
    "    print(\"'{}'\".format(exp_id))\n",
    "    for kk in exps[exp_id]:\n",
    "        print(np.max(exps[exp_id][kk][0]['epoch']))\n",
    "        print(exps[exp_id][kk][4])\n",
    "\n",
    "    resO = collect_results(exps[exp_id], ('ap', 'ap'), 0, None)\n",
    "\n",
    "    for alg in resO:\n",
    "        res_test_str = \"ES_{} \".format(alg) +print_row(resO[alg]['test'], ('ap', 'ap'))\n",
    "        res_id_t_str = \"ES_{} \".format(alg) +print_row(resO[alg]['id_test'], ('ap', 'ap'))\n",
    "        res_train_str = \"ES_{} \".format(alg) +print_row(resO[alg]['train'], ('ap', 'ap'))\n",
    "\n",
    "        print(\"Te\\t\", res_test_str)\n",
    "        #print(\"ID \\t\", res_id_t_str)\n",
    "        print(\"Tr \\t\", res_train_str)\n",
    "        break\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "spectacular-lambda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exps.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aboriginal-polymer",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "informed-retrieval",
   "metadata": {},
   "source": [
    "# FMOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "hidden-seafood",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dicts = sorted(glob.glob('./AMASK_*/'), key= lambda x:int(x.split('_')[-1].split('/')[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "uniform-tumor",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_keys = ['Dataset', 'Algorithm', 'Uniform over groups', 'Distinct groups', 'N groups per batch']\n",
    "my_keys += ['Batch size', 'Rd type', 'Warm start epoch', 'Control only direction', 'Only inconsistent']\n",
    "my_keys += ['Without sampling', 'Lr', 'Weight decay', 'Seed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "previous-charge",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./AMASK_16/  0.0005\n",
      "./AMASK_17/  0.0005\n",
      "./AMASK_18/  0.0005\n"
     ]
    }
   ],
   "source": [
    "exps = {}\n",
    "for log_di in log_dicts:\n",
    "    dat_conf = parse_config(log_di, my_keys)\n",
    "    if not dat_conf['Dataset'] == ' fmow':\n",
    "        continue\n",
    "    print(log_di, dat_conf['Lr'])\n",
    "    exp_name = \"{}_{}_{}_{}_{}\".format(dat_conf['Warm start epoch'], dat_conf['Rd type'], \n",
    "                                 dat_conf['Batch size'], dat_conf['N groups per batch'],\n",
    "                                 dat_conf['Batch size'])\n",
    "    if exp_name not in exps:\n",
    "        exps[exp_name] = {}\n",
    "    exps[exp_name][dat_conf['Seed']] = (read_to_dict(log_di+'test_eval.csv'), \n",
    "                                        read_to_dict(log_di+'val_eval.csv'), \n",
    "                                        read_to_dict(log_di+'id_test_eval.csv'),\n",
    "                                        read_to_dict(log_di+'train_eval.csv'),\n",
    "                                        log_di)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ordered-michael",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "' -1_ 42_ 64_ 8_ 64'\n",
      "25.0\n",
      "./AMASK_16/\n",
      "29.0\n",
      "./AMASK_17/\n",
      "29.0\n",
      "./AMASK_18/\n",
      "Te\t ES_0  26.35 1.36 15.67 0.42\n",
      "ID \t ES_0  31.62 1.42 28.04 1.53\n",
      "Tr \t ES_0  39.47 1.63 29.27 1.64\n",
      "Te\t ES_1  25.46 0.66 17.70 0.75\n",
      "ID \t ES_1  30.81 1.15 27.85 1.77\n",
      "Tr \t ES_1  37.92 1.77 27.85 2.63\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for exp_id in exps:\n",
    "    print(\"'{}'\".format(exp_id))\n",
    "    for kk in exps[exp_id]:\n",
    "        print(np.max(exps[exp_id][kk][0]['epoch']))\n",
    "        print(exps[exp_id][kk][4])\n",
    "\n",
    "    resO = collect_results(exps[exp_id], ('acc_avg', 'acc_worst_region'), 0, None)\n",
    "\n",
    "    for alg in resO:\n",
    "        res_test_str = \"ES_{} \".format(alg) +print_row(resO[alg]['test'], ('acc_avg', 'acc_worst_region'))\n",
    "        res_id_t_str = \"ES_{} \".format(alg) +print_row(resO[alg]['id_test'], ('acc_avg', 'acc_worst_region'))\n",
    "        res_train_str = \"ES_{} \".format(alg) +print_row(resO[alg]['train'], ('acc_avg', 'acc_worst_region'))\n",
    "\n",
    "        print(\"Te\\t\", res_test_str)\n",
    "        print(\"ID \\t\", res_id_t_str)\n",
    "        print(\"Tr \\t\", res_train_str)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bacterial-organizer",
   "metadata": {},
   "source": [
    "# Cam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "streaming-eugene",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dicts = sorted(glob.glob('./AMASK_*/'), key= lambda x:int(x.split('_')[-1].split('/')[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "adequate-banks",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_keys = ['Dataset', 'Algorithm', 'Uniform over groups', 'Distinct groups', 'N groups per batch']\n",
    "my_keys += ['Batch size', 'Rd type', 'Warm start epoch', 'Control only direction', 'Only inconsistent']\n",
    "my_keys += ['Without sampling', 'Lr', 'Weight decay', 'Seed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "minor-outreach",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./AMASK_3/  0.005\n",
      "./AMASK_4/  0.005\n",
      "./AMASK_5/  0.005\n",
      "./AMASK_6/  0.005\n",
      "./AMASK_7/  0.005\n",
      "./AMASK_8/  0.005\n",
      "./AMASK_9/  0.005\n",
      "./AMASK_10/  0.005\n",
      "./AMASK_11/  0.005\n",
      "./AMASK_12/  0.005\n"
     ]
    }
   ],
   "source": [
    "exps = {}\n",
    "for log_di in log_dicts:\n",
    "    dat_conf = parse_config(log_di, my_keys)\n",
    "    if not dat_conf['Dataset'] == ' camelyon17':\n",
    "        continue\n",
    "    print(log_di, dat_conf['Lr'])\n",
    "    exp_name = \"{}_{}_{}_{}_{}\".format(dat_conf['Warm start epoch'], dat_conf['Rd type'], \n",
    "                                 dat_conf['Lr'], dat_conf['N groups per batch'],\n",
    "                                 dat_conf['Batch size'])\n",
    "    if exp_name not in exps:\n",
    "        exps[exp_name] = {}\n",
    "    exps[exp_name][dat_conf['Seed']] = (read_to_dict(log_di+'test_eval.csv'), \n",
    "                                        read_to_dict(log_di+'val_eval.csv'), \n",
    "                                        read_to_dict(log_di+'id_val_eval.csv'),\n",
    "                                        read_to_dict(log_di+'train_eval.csv'),\n",
    "                                        log_di)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "republican-jurisdiction",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "' -1_ 42_ 0.005_ 3_ 30'\n",
      "4.0\n",
      "./AMASK_3/\n",
      "4.0\n",
      "./AMASK_4/\n",
      "4.0\n",
      "./AMASK_5/\n",
      "4.0\n",
      "./AMASK_6/\n",
      "4.0\n",
      "./AMASK_7/\n",
      "4.0\n",
      "./AMASK_8/\n",
      "4.0\n",
      "./AMASK_9/\n",
      "4.0\n",
      "./AMASK_10/\n",
      "4.0\n",
      "./AMASK_11/\n",
      "4.0\n",
      "./AMASK_12/\n",
      "Te\t ES_0  70.24 10.92 48.90 12.66\n",
      "Te\t ES_1  69.05 9.90 47.99 11.41\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for exp_id in exps:\n",
    "    print(\"'{}'\".format(exp_id))\n",
    "    skip_exp = False\n",
    "    for kk in exps[exp_id]:\n",
    "        print(np.max(exps[exp_id][kk][0]['epoch']))\n",
    "        print(exps[exp_id][kk][4])\n",
    "    if skip_exp:\n",
    "        continue\n",
    "        \n",
    "    resO = collect_results(exps[exp_id], ('acc_avg', 'acc_wg'), 1, None)\n",
    "\n",
    "    for alg in resO:\n",
    "        res_test_str = \"ES_{} \".format(alg) +print_row(resO[alg]['test'], ('acc_avg', 'acc_wg'))\n",
    "        res_id_t_str = \"ES_{} \".format(alg) +print_row(resO[alg]['id_test'], ('acc_avg', 'acc_wg'))\n",
    "        res_train_str = \"ES_{} \".format(alg) +print_row(resO[alg]['train'], ('acc_avg', 'acc_wg'))\n",
    "\n",
    "        print(\"Te\\t\", res_test_str)\n",
    "        #print(\"ID \\t\", res_id_t_str)\n",
    "        #print(\"Tr \\t\", res_train_str)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "planned-imperial",
   "metadata": {},
   "source": [
    "# Poverty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "chief-wallpaper",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dicts = sorted(glob.glob('./AMASK_*/'), key= lambda x:int(x.split('_')[-1].split('/')[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "checked-aviation",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_keys = ['Dataset', 'Algorithm', 'Uniform over groups', 'Distinct groups', 'N groups per batch']\n",
    "my_keys += ['Batch size', 'Rd type', 'Warm start epoch', 'Control only direction', 'Only inconsistent']\n",
    "my_keys += ['Without sampling', 'Lr', 'Weight decay', 'Seed', 'Dataset kwargs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "naval-immune",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "capable-offset",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./AMASK_22/  0.0005\n",
      "./AMASK_23/  0.0005\n",
      "./AMASK_24/  0.0005\n",
      "./AMASK_25/  0.0005\n",
      "./AMASK_26/  0.0005\n"
     ]
    }
   ],
   "source": [
    "exps = {}\n",
    "for log_di in log_dicts:\n",
    "    #if 'RDE' not in log_di:\n",
    "    #    continue\n",
    "    dat_conf = parse_config(log_di, my_keys)\n",
    "    if not dat_conf['Dataset'] == ' poverty':\n",
    "        continue\n",
    "    print(log_di, dat_conf['Lr'])\n",
    "    exp_name = \"{}_{}_{}_{}_{}\".format(dat_conf['Warm start epoch'], dat_conf['Rd type'], \n",
    "                                 dat_conf['Lr'], dat_conf['Lr'],\n",
    "                                 dat_conf['Batch size'])\n",
    "    if exp_name not in exps:\n",
    "        exps[exp_name] = {}\n",
    "    exps[exp_name][dat_conf['Dataset kwargs']] = (read_to_dict(log_di+'test_eval.csv'), \n",
    "                                        read_to_dict(log_di+'val_eval.csv'), \n",
    "                                        None,\n",
    "                                        read_to_dict(log_di+'id_val_eval.csv'),\n",
    "                                        log_di)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "otherwise-mouth",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "' -1_ 42_ 0.0005_ 0.0005_ 64'\n",
      "164.0\n",
      "./AMASK_22/\n",
      "166.0\n",
      "./AMASK_23/\n",
      "166.0\n",
      "./AMASK_24/\n",
      "173.0\n",
      "./AMASK_25/\n",
      "171.0\n",
      "./AMASK_26/\n",
      "OOD_0  80.86 3.85 50.44 4.11\n",
      "ID_0  0nan 0nan 0nan 0nan\n",
      "TR_0  81.19 1.57 54.04 2.09\n",
      "VAL_0  79.59 2.88 46.97 4.94\n",
      "OOD_1  80.33 3.94 51.45 4.03\n",
      "ID_1  0nan 0nan 0nan 0nan\n",
      "TR_1  81.34 1.29 55.09 1.65\n",
      "VAL_1  78.53 3.68 44.52 6.03\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for exp_id in exps:\n",
    "    print(\"'{}'\".format(exp_id))\n",
    "    for kk in exps[exp_id]:\n",
    "        print(np.max(exps[exp_id][kk][0]['epoch']))\n",
    "        print(exps[exp_id][kk][4])\n",
    "        \n",
    "    resO = collect_results(exps[exp_id], ('r_all', 'r_wg'), 0, None)\n",
    "\n",
    "    for alg in resO:\n",
    "        res_test_str = \"OOD_{} \".format(alg) +print_row(resO[alg]['test'], ('r_all', 'r_wg'))\n",
    "        res_id_t_str = \"ID_{} \".format(alg) +print_row(resO[alg]['id_test'], ('r_all', 'r_wg'))\n",
    "        res_train_str = \"TR_{} \".format(alg) +print_row(resO[alg]['train'], ('r_all', 'r_wg'))\n",
    "        res_val_str = \"VAL_{} \".format(alg) +print_row(resO[alg]['val'], ('r_all', 'r_wg'))\n",
    "\n",
    "        print(res_test_str)\n",
    "        print(res_id_t_str)\n",
    "        print(res_train_str)\n",
    "        print(res_val_str)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "correct-blackberry",
   "metadata": {},
   "source": [
    "# Amazon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "stable-plane",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dicts = sorted(glob.glob('./AMASK_*/'), key= lambda x:int(x.split('_')[-1].split('/')[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "leading-think",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_keys = ['Dataset', 'Algorithm', 'Uniform over groups', 'Distinct groups', 'N groups per batch']\n",
    "my_keys += ['Batch size', 'Rd type', 'Warm start epoch', 'Control only direction', 'Only inconsistent']\n",
    "my_keys += ['Without sampling', 'Lr', 'Weight decay', 'Seed', 'Dataset kwargs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "favorite-hayes",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./AMASK_19/  1e-05\n",
      "./AMASK_20/  1e-05\n",
      "./AMASK_21/  1e-05\n"
     ]
    }
   ],
   "source": [
    "exps = {}\n",
    "for log_di in log_dicts:\n",
    "    #if 'RDE' not in log_di:\n",
    "    #    continue\n",
    "    dat_conf = parse_config_orig(log_di, my_keys)\n",
    "    if not dat_conf['Dataset'] == ' amazon':\n",
    "        continue\n",
    "    print(log_di, dat_conf['Lr'])\n",
    "    exp_name = \"{}_{}_{}_{}_{}\".format(dat_conf['Warm start epoch'], dat_conf['Rd type'], \n",
    "                                 dat_conf['Lr'], dat_conf['N groups per batch'],\n",
    "                                 dat_conf['Batch size'])\n",
    "    if exp_name not in exps:\n",
    "        exps[exp_name] = {}\n",
    "    test_res = read_to_dict(log_di+'test_eval.csv')\n",
    "    if 'epoch' not in test_res:\n",
    "        continue\n",
    "    exps[exp_name][dat_conf['Seed']] = (test_res, \n",
    "                                        read_to_dict(log_di+'val_eval.csv'), \n",
    "                                        read_to_dict(log_di+'id_test_eval.csv'),\n",
    "                                        read_to_dict(log_di+'train_eval.csv'),\n",
    "                                        log_di)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "timely-myrtle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "' -1_ 42_ 1e-05_ 2_ 8'\n",
      "2.0\n",
      "./AMASK_19/\n",
      "2.0\n",
      "./AMASK_20/\n",
      "2.0\n",
      "./AMASK_21/\n",
      "OOD_0  70.30 0.39 51.56 0.63\n",
      "OOD_1  69.44 0.83 52.00 0.00\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for exp_id in exps:\n",
    "    print(\"'{}'\".format(exp_id))\n",
    "    breakl = False\n",
    "    for kk in exps[exp_id]:\n",
    "        if 'epoch' not in exps[exp_id][kk][0]:\n",
    "            breakl = True\n",
    "            continue\n",
    "        print(np.max(exps[exp_id][kk][0]['epoch']))\n",
    "        print(exps[exp_id][kk][4])\n",
    "    #if breakl:\n",
    "    #    continue\n",
    "    resO = collect_results(exps[exp_id], ('acc_avg', '10th_percentile_acc'), 0, None)\n",
    "\n",
    "    for alg in resO:\n",
    "        res_test_str = \"OOD_{} \".format(alg) +print_row(resO[alg]['test'], ('acc_avg', '10th_percentile_acc'))\n",
    "        res_id_t_str = \"ID_{} \".format(alg) +print_row(resO[alg]['id_test'], ('acc_avg', '10th_percentile_acc'))\n",
    "        res_train_str = \"TR_{} \".format(alg) +print_row(resO[alg]['train'], ('acc_avg', '10th_percentile_acc'))\n",
    "        res_val_str = \"VAL_{} \".format(alg) +print_row(resO[alg]['val'], ('acc_avg', '10th_percentile_acc'))\n",
    "\n",
    "        print(res_test_str)\n",
    "        #print(res_id_t_str)\n",
    "        #print(res_train_str)\n",
    "        #print(res_val_str)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "posted-supply",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
